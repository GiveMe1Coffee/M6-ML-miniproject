---
title: "M6 : Machine Learning Miniproject"
author: "Léo Besançon & Larissa Geiser"
date: '`r Sys.Date()`'
output: word_document
---

# 1. Data loading and description

```{r library, echo=FALSE}
#################### LIBRARY 

#install.packages('rattle')
library(dplyr)
library(ggplot2)
library(psych)
library(rattle)
library(rpart)
library(stats)
library(tidyr)
library(umap)

set.seed(123)
```

```{r data import}
#################### DATA IMPORTING AND DESCRIPTION

setwd('/Users/larissageiser/Documents/GitHub/M6-ML-miniproject') 
# Lari : '/Users/larissageiser/Documents/GitHub/M6-ML-miniproject'
data <- read.csv('ObesityDataSet_raw_and_data_sinthetic.csv')
dim(data)
sum(is.na(data))
head(data)
```
The data comes from more than 2'000 individuals. It consists of an evaluation of 16 physical and behavioral attributes. The last column indicates the weight class of the individual.

## 1.1. Data harmonization

To be able to use this dataset with the different algorithms, we need to modify the variables encoded with characters into numbers.

```{r data harmonization}
#################### DATA HARMONIZATION

# gender : 0 = female, 1 = male
data$Gender[data$Gender == 'Female'] <- 0
data$Gender[data$Gender == 'Male'] <- 1
data$Gender <- as.numeric(data$Gender)

# family history with overweight
data$family_history_with_overweight[data$family_history_with_overweight == 'no'] <- 0
data$family_history_with_overweight[data$family_history_with_overweight == 'yes'] <- 1
data$family_history_with_overweight <- as.numeric(data$family_history_with_overweight)

# FAVC (frequency of highly caloric food)
data$FAVC[data$FAVC == 'no'] <- 0
data$FAVC[data$FAVC == 'yes'] <- 1
data$FAVC <- as.numeric(data$FAVC)

# CAEC (snacking between meals)
data$CAEC[data$CAEC == 'no'] <- 0
data$CAEC[data$CAEC == 'Sometimes'] <- 1
data$CAEC[data$CAEC == 'Frequently'] <- 2
data$CAEC[data$CAEC == 'Always'] <- 3
data$CAEC <- as.numeric(data$CAEC)

# smoke
data$SMOKE[data$SMOKE == 'no'] <- 0
data$SMOKE[data$SMOKE == 'yes'] <- 1
data$SMOKE <- as.numeric(data$SMOKE)

# SCC (calories monitoring)
data$SCC[data$SCC == 'no'] <- 0
data$SCC[data$SCC == 'yes'] <- 1
data$SCC <- as.numeric(data$SMOKE)

# CALC (alcohol consumption)
data$CALC[data$CALC == 'no'] <- 0
data$CALC[data$CALC == 'Sometimes'] <- 1
data$CALC[data$CALC == 'Frequently'] <- 2
data$CALC[data$CALC == 'Always'] <- 3
data$CALC <- as.numeric(data$CALC)

# MTRANS (mode of transportation)
data$MTRANS <- as.factor(data$MTRANS) # better to encode with numbers ? i don't think so

# NObeyesdad (obesity)
data$NObeyesdad <- as.factor(data$NObeyesdad)
colnames(data)[colnames(data) == 'NObeyesdad'] <- 'Obesity'

# results
head(data)

```


# 2. 1st technique : k-means clustering

## 2.1. Data visualisation

We use the ´psych´ package to visualize the data

```{r k-means visualisation}
#################### DATA VISUALISATION

pairs.panels(data[1:16],
            ellipses = F,
            pch= 21,
            bg = data$Obesity)

```
Visually, the best segregating variables are ´weight´ and ´height´


## 2.2. K-means algorithm
We use the k-means algorithm from the ´stats´ package. We verify if the clustering works with a confusion matrix. 

```{r kmeans algorithm}
#################### K-MEANS ALGORITHM

datakmeans <- data.frame(data$Weight, data$Height, data$Obesity)

# call kmeans again but this time passing the centers calculated in the previous step
km <- kmeans(datakmeans[,1:2], 7)

# plot of the results
plot(datakmeans[1:2], 
     col=km$cluster, 
     pch=19)
legend(165, 1.8, 
       c("1","2", "3", "4", "5", "6", "7"),
       pch=19, 
       col=c(1:7))

```


```{r kmeans accuracy}
#################### K-MEANS ACCURACY

# re-order kmeans clusters according to mean weight


# confusion matrix
cm <- table(label=data$Obesity, cluster=km$cluster)
cm ; cat( sum(diag(cm)) / sum(cm) )

```


# 3. 2nd technique : Decision tree

## 3.1. separate train and test sets

```{r decision tree sets}
#################### TRAIN AND TEST SETS

n <- nrow(data)
sel <- sort(sample.int(n, n/4))
data.train <- data[-sel,]
data.test <- data[sel,]

```

```{r decision tree algorithm}
#################### DECISION TREE ALGORITHM

h <- rpart(Obesity ~ . , data = data.train, method ='class', cp=0, xval=500)

fancyRpartPlot(h, caption = NULL, type = 1)

```

```{r truncate tree}
#################### DECISION TREE PRUNING

plotcp(h)

h_pruned <- prune(h, cp=0.03)

fancyRpartPlot(h_pruned, caption=NULL, type=2)
```

```{r tree evaluation}
#################### DECISION TREE EVALUATION

obesity_pred <- predict(h_pruned, data.test, type= 'class')

conf_table <- table(true=data.test$Obesity, predicted = obesity_pred)

n <- sum(conf_table)
error = (n - sum(diag(conf_table)))/n

cat(sprintf("The relative prediction error is %4.1f%%",error*100))
```

