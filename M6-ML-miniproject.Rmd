---
title: "M6 : Machine Learning Miniproject"
author: "Léo Besançon & Larissa Geiser"
date: '`r Sys.Date()`'
output: word_document
---

# 1. Data loading and description

```{r library, include=FALSE}
#################### LIBRARY 

#install.packages('')
library(dplyr)
library(ggplot2)
library(psych)
library(rattle)
library(rpart)
library(stats)
library(tidyr)
library(umap)

set.seed(123)
```

```{r setup lari, echo=FALSE}
#################### DATA IMPORTING for Larissa
setwd('/Users/larissageiser/Documents/GitHub/M6-ML-miniproject') 
```

```{r setup leo, echo = FALSE}
#################### DATA IMPORTING for Leo
#setwd('C:/Users/besan/OneDrive/Bureau/Module 6/Machine learning') 
```

```{r read data}
data <- read.csv('ObesityDataSet_raw_and_data_sinthetic.csv')
dim(data)
sum(is.na(data)) #no missing value
head(data)
```

The data comes from more than 2'000 individuals. It consists of an evaluation of 16 physical and behavioral attributes. The last column indicates the weight class of the individual.

## 1.1. Data harmonization

To be able to use this dataset with the different algorithms, we need to modify the variables encoded with characters into numbers.

```{r data harmonization}
#################### DATA HARMONIZATION

# gender : 0 = female, 1 = male
data$Gender[data$Gender == 'Female'] <- 0
data$Gender[data$Gender == 'Male'] <- 1
data$Gender <- as.numeric(data$Gender)

# family history with overweight
data$family_history_with_overweight[data$family_history_with_overweight == 'no'] <- 0
data$family_history_with_overweight[data$family_history_with_overweight == 'yes'] <- 1
data$family_history_with_overweight <- as.numeric(data$family_history_with_overweight)

# FAVC (frequency of highly caloric food)
data$FAVC[data$FAVC == 'no'] <- 0
data$FAVC[data$FAVC == 'yes'] <- 1
data$FAVC <- as.numeric(data$FAVC)

# CAEC (snacking between meals)
data$CAEC[data$CAEC == 'no'] <- 0
data$CAEC[data$CAEC == 'Sometimes'] <- 1
data$CAEC[data$CAEC == 'Frequently'] <- 2
data$CAEC[data$CAEC == 'Always'] <- 3
data$CAEC <- as.numeric(data$CAEC)

# smoke
data$SMOKE[data$SMOKE == 'no'] <- 0
data$SMOKE[data$SMOKE == 'yes'] <- 1
data$SMOKE <- as.numeric(data$SMOKE)

# SCC (calories monitoring)
data$SCC[data$SCC == 'no'] <- 0
data$SCC[data$SCC == 'yes'] <- 1
data$SCC <- as.numeric(data$SMOKE)

# CALC (alcohol consumption)
data$CALC[data$CALC == 'no'] <- 0
data$CALC[data$CALC == 'Sometimes'] <- 1
data$CALC[data$CALC == 'Frequently'] <- 2
data$CALC[data$CALC == 'Always'] <- 3
data$CALC <- as.numeric(data$CALC)

# MTRANS (mode of transportation)
data$MTRANS <- as.factor(data$MTRANS) # better to encode with numbers ? i don't think so

# NObeyesdad (obesity)
data$NObeyesdad <- as.factor(data$NObeyesdad)
colnames(data)[colnames(data) == 'NObeyesdad'] <- 'Obesity'

# results
head(data)

```


# 2. 1st technique : k-means clustering

We think that K-means clustering is a good technique for this dataset. We already know that we have 7 categories for obesity, so it is interesting to see if the technique is able to separate the data into the right clusters.

## 2.1. Data visualisation

We use the ´psych´ package to visualize the data

```{r k-means visualisation}
#################### DATA VISUALISATION

pairs.panels(data[1:16],
            ellipses = F,
            pch= 21,
            bg = data$Obesity)

```
The best segregating variables seem to be ´weight´ and ´height´


## 2.2. K-means algorithm
We use the k-means algorithm from the ´stats´ package.

```{r kmeans algorithm}
#################### K-MEANS ALGORITHM

datakmeans <- data.frame(data$Weight, data$Height, data$Obesity)

# call kmeans again but this time passing the centers calculated in the previous step
km <- kmeans(datakmeans[,1:2], 7)

# plot of the results
plot(datakmeans[1:2], 
     col=km$cluster, 
     pch=19)
legend(165, 1.8, 
       c("1","2", "3", "4", "5", "6", "7"),
       pch=19, 
       col=c(1:7))

```

The clusters aren't in the right order. We modify the cluster "names" to then measure accuracy :

```{r kmeans cluster ordering}
#################### K-MEANS CLUSTER ORDERING

# re-order kmeans clusters according to mean weight
ordered_center <- order(km$centers[,1])
ordered_labels <- match(km$cluster,ordered_center)
# plot of the results
plot(datakmeans[1:2], 
     col=ordered_labels, 
     pch=19)
legend(165, 1.8, 
       c("1","2", "3", "4", "5", "6", "7"),
       pch=19, 
       col=c(1:7))

```

The clusers are now in the right order. We measure the accuracy of the technique with a confusion matrix :

```{r kmeans confusion matrix}
#################### K-MEANS CONFUSION MATRIX
cm <- table(label=data$Obesity, cluster=ordered_labels)
cm ; cat( sum(diag(cm)) / sum(cm) )
```

The technique is not very accurate, only 21% of the clustering is right.

# 3. 2nd technique : Decision tree

## 3.1. Separate train and test sets

```{r decision tree sets}
#################### TRAIN AND TEST SETS

n <- nrow(data)
sel <- sort(sample.int(n, n/4))
data.train <- data[-sel,]
data.test <- data[sel,]

```

```{r decision tree algorithm}
#################### DECISION TREE ALGORITHM

h <- rpart(Obesity ~ . , data = data.train, method ='class', cp=0, xval=500)

fancyRpartPlot(h, caption = NULL, type = 1)

```

The tree is too big, we need to truncate it. We adjusted the cp value until we had 7 categories.

```{r truncate tree}
#################### DECISION TREE PRUNING

plotcp(h)

h_pruned <- prune(h, cp=0.03)

fancyRpartPlot(h_pruned, caption=NULL, type=2)
```

The second tree looks better, and we have indeed our 7 categories. We can now measure its accuracy :

```{r tree evaluation}
#################### DECISION TREE EVALUATION

obesity_pred <- predict(h_pruned, data.test, type= 'class')

conf_table <- table(true=data.test$Obesity, predicted = obesity_pred)

n <- sum(conf_table)
error = (n - sum(diag(conf_table)))/n

cat(sprintf("The relative prediction error is %4.1f%%",error*100))
```

For such a dataset and mini-project, an error of ´{r} error*100´% is not that bad.